# R-Chat ðŸš€

R-Chat is a modern chat application built with Next.js that allows users to chat in real-time, manage multiple chat sessions, and interact with intelligent assistant responses using both local models (via Ollama) and API-based models.

<!-- Screenshots -->
## Interface

![Interface](https://github.com/user-attachments/assets/88d0ba68-3b08-4a73-a04c-2677b6b66346) 

# Features

- **Switch Between Models:** Easily switch between local and API models ðŸ”„
  
  ![Switch Models](https://github.com/user-attachments/assets/575231c8-a587-4d59-a08c-7b7009d4db81) 

- **Custom API Models:** Add your own API models effortlessly ðŸ› ï¸
  
  ![Custom API](https://github.com/user-attachments/assets/3212c2c2-ff24-44eb-aeff-957316203261) 

- **Multiple Providers:** Choose between different providers (more options coming soon!) ðŸŒ
  
  ![Providers](https://github.com/user-attachments/assets/24de4110-ac10-4e81-811f-695ca8053175) 

- **Local Chat History:** All chat history is stored locally, ensuring your privacy ðŸ”’
  
  ![Local Chat](https://github.com/user-attachments/assets/595925df-8329-48b9-9ca6-fcae38abf8df)
 

- **Efficient Storage Management:** Monitor and clear chat storage easily ðŸ—‘ï¸
  
  ![Storage](https://github.com/user-attachments/assets/2b123f6d-6ed9-42b0-b510-15a3829371d0) 

- **Dynamic Reasoning:** Enjoy a neat bubble for the thinking process (Reasoning Models)ðŸ’¡
  
  ![Reasoning Model](https://github.com/user-attachments/assets/12c08cc9-1fa1-44f4-aa7e-825f35d1e624) 

- **Toggle Reasoning View:** Prefer a cleaner interface? Hide the reasoning process with a click! ðŸ˜Š
    
  ![Toggle Reasoning](https://github.com/user-attachments/assets/9e193016-f410-494e-9a5b-0d3d18065212)

## Getting Started

### Prerequisites
- Node.js (v14 or above)
- npm, yarn, or pnpm

### Installation & Setup
1. **Clone the Repository:**
   ```bash
   git clone https://github.com/hanish-rishen/R-Chat.git
   cd r-chat
   ```
2. **Install Dependencies:**
   ```bash
   npm install
   # or yarn install
   # or pnpm install
   ```
3. **Run Local Models (if you have):**
   > Open a separate terminal and run:
   ```bash
   ollama serve
   ```
4. **Start the Development Server:**
   ```bash
   npm run dev
   # or yarn dev
   # or pnpm dev
   ```
5. **Access the Application:**
   Open [http://localhost:3000](http://localhost:3000) in your browser.


